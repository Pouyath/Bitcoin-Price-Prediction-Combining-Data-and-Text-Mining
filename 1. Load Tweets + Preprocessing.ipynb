{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "        \n",
    "    '''\n",
    "    Utility function to classify sentiment of passed tweet\n",
    "    using textblob's sentiment method\n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text\n",
    "    analysis = TextBlob(tweet)\n",
    "\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def truncate_time(time):\n",
    "    return time.replace(hour=0 ,minute=0, second=0, microsecond=0)\n",
    "\n",
    "def emoticons(tweet):\n",
    "    if ':)' in tweet:\n",
    "        return 1\n",
    "    elif ':(' in tweet:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def alot_likes(tweet):\n",
    "    if tweet>10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()  \n",
    "def vader(tweet):  \n",
    "    ss = sid.polarity_scores(tweet)   \n",
    "    return (ss['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First import everything I have from the days (Fill the gaps of the weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12457\n",
      "15693\n",
      "17942\n",
      "19270\n",
      "17310\n",
      "35487\n",
      "28723\n",
      "1552\n",
      "18410\n",
      "12667\n",
      "40330\n",
      "33427\n"
     ]
    }
   ],
   "source": [
    "list_ = []\n",
    "for i in range(1,13):\n",
    "    try:\n",
    "        df = pd.read_json('Data/By Day/2017'+str(i)+'.json', lines = True, convert_dates = False)\n",
    "        print (len(df))\n",
    "        list_.append(df)\n",
    "    except:\n",
    "        print ('Problem with :',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everithing I have from Weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58337\n",
      "104261\n",
      "137591\n",
      "Problem with : 4\n",
      "117920\n",
      "124157\n",
      "78228\n",
      "86117\n",
      "78722\n",
      "158893\n",
      "34029\n",
      "107063\n",
      "107669\n",
      "129875\n",
      "102954\n",
      "118863\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,17):\n",
    "    try:\n",
    "        df = pd.read_json('Data/By Week/week'+str(i)+'.json', lines = True, convert_dates = False)\n",
    "        print (len(df))\n",
    "        list_.append(df)\n",
    "    except:\n",
    "        print ('Problem with :',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621642\n",
      "610130\n",
      "270958\n",
      "136468\n",
      "776692\n",
      "802065\n",
      "1167790\n",
      "1406214\n",
      "1089825\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,14):\n",
    "    try:\n",
    "        df = pd.read_json('Data/By month/month'+str(i)+'.json', lines = True, convert_dates = False)\n",
    "        print (len(df))\n",
    "        list_.append(df)\n",
    "    except:\n",
    "        print ('Problem with :',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import fill Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828852\n",
      "37332\n",
      "194235\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"Data/tweets3.json\", encoding =\"utf-8\", lines=True, convert_dates = False)\n",
    "print (len(df))\n",
    "list_.append(df)\n",
    "df = pd.read_json(\"Data/fill0.json\", encoding =\"utf-8\", lines=True, convert_dates = False)\n",
    "print (len(df))\n",
    "list_.append(df)\n",
    "df = pd.read_json(\"Data/fill.json\", encoding =\"utf-8\", lines=True, convert_dates = False)\n",
    "print (len(df))\n",
    "list_.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Them all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9740150\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.concat(list_)\n",
    "print (len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8808386\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.drop_duplicates()\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See how much of each date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12457\n",
      "15693\n",
      "17942\n",
      "19217\n",
      "23085\n",
      "22120\n",
      "18487\n",
      "10353\n",
      "5745\n",
      "0        2017-01-09\n",
      "1        2017-01-09\n",
      "2        2017-01-09\n",
      "3        2017-01-09\n",
      "4        2017-01-09\n",
      "5        2017-01-09\n",
      "6        2017-01-09\n",
      "7        2017-01-09\n",
      "8        2017-01-09\n",
      "9        2017-01-09\n",
      "10       2017-01-09\n",
      "11       2017-01-09\n",
      "12       2017-01-09\n",
      "13       2017-01-09\n",
      "14       2017-01-09\n",
      "15       2017-01-09\n",
      "16       2017-01-09\n",
      "17       2017-01-09\n",
      "18       2017-01-09\n",
      "19       2017-01-09\n",
      "20       2017-01-09\n",
      "21       2017-01-09\n",
      "22       2017-01-09\n",
      "23       2017-01-09\n",
      "24       2017-01-09\n",
      "25       2017-01-09\n",
      "26       2017-01-09\n",
      "27       2017-01-09\n",
      "28       2017-01-09\n",
      "29       2017-01-09\n",
      "            ...    \n",
      "17607    2017-01-09\n",
      "17608    2017-01-09\n",
      "17609    2017-01-09\n",
      "17610    2017-01-09\n",
      "17611    2017-01-09\n",
      "17612    2017-01-09\n",
      "17613    2017-01-09\n",
      "17614    2017-01-09\n",
      "17615    2017-01-09\n",
      "17616    2017-01-09\n",
      "17617    2017-01-09\n",
      "17618    2017-01-09\n",
      "17619    2017-01-09\n",
      "17620    2017-01-09\n",
      "17621    2017-01-09\n",
      "17622    2017-01-09\n",
      "17623    2017-01-09\n",
      "17624    2017-01-09\n",
      "17625    2017-01-09\n",
      "17626    2017-01-09\n",
      "17627    2017-01-09\n",
      "17628    2017-01-09\n",
      "17629    2017-01-09\n",
      "17630    2017-01-09\n",
      "17631    2017-01-09\n",
      "18146    2017-01-09\n",
      "18254    2017-01-09\n",
      "18324    2017-01-09\n",
      "18332    2017-01-09\n",
      "18348    2017-01-09\n",
      "Name: date, Length: 5745, dtype: object\n",
      "17217\n",
      "25297\n",
      "20038\n",
      "18006\n",
      "14931\n",
      "9987\n",
      "0       2017-01-15\n",
      "1       2017-01-15\n",
      "2       2017-01-15\n",
      "3       2017-01-15\n",
      "4       2017-01-15\n",
      "5       2017-01-15\n",
      "6       2017-01-15\n",
      "7       2017-01-15\n",
      "8       2017-01-15\n",
      "9       2017-01-15\n",
      "10      2017-01-15\n",
      "11      2017-01-15\n",
      "12      2017-01-15\n",
      "13      2017-01-15\n",
      "14      2017-01-15\n",
      "15      2017-01-15\n",
      "16      2017-01-15\n",
      "17      2017-01-15\n",
      "18      2017-01-15\n",
      "19      2017-01-15\n",
      "20      2017-01-15\n",
      "21      2017-01-15\n",
      "22      2017-01-15\n",
      "23      2017-01-15\n",
      "24      2017-01-15\n",
      "25      2017-01-15\n",
      "26      2017-01-15\n",
      "27      2017-01-15\n",
      "28      2017-01-15\n",
      "29      2017-01-15\n",
      "           ...    \n",
      "9957    2017-01-15\n",
      "9958    2017-01-15\n",
      "9959    2017-01-15\n",
      "9960    2017-01-15\n",
      "9961    2017-01-15\n",
      "9962    2017-01-15\n",
      "9963    2017-01-15\n",
      "9964    2017-01-15\n",
      "9965    2017-01-15\n",
      "9966    2017-01-15\n",
      "9967    2017-01-15\n",
      "9968    2017-01-15\n",
      "9969    2017-01-15\n",
      "9970    2017-01-15\n",
      "9971    2017-01-15\n",
      "9972    2017-01-15\n",
      "9973    2017-01-15\n",
      "9974    2017-01-15\n",
      "9975    2017-01-15\n",
      "9976    2017-01-15\n",
      "9977    2017-01-15\n",
      "9978    2017-01-15\n",
      "9979    2017-01-15\n",
      "9980    2017-01-15\n",
      "9981    2017-01-15\n",
      "9982    2017-01-15\n",
      "9983    2017-01-15\n",
      "9984    2017-01-15\n",
      "9985    2017-01-15\n",
      "9986    2017-01-15\n",
      "Name: date, Length: 9987, dtype: object\n",
      "19494\n",
      "18450\n",
      "18218\n",
      "18925\n",
      "15244\n",
      "16375\n",
      "16731\n",
      "19980\n",
      "16976\n",
      "17490\n",
      "19074\n",
      "18486\n",
      "13256\n",
      "14096\n",
      "16233\n",
      "17382\n",
      "0\n",
      "Series([], Name: date, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "df = tweets \n",
    "for i in range (1,33):\n",
    "    if i in [1,2,3,4,5,6,7,8,9] :\n",
    "        df = tweets \n",
    "        condition = df['date'] == '2017-01-0'+str(i)\n",
    "        df = df[condition]\n",
    "        print (len(df))\n",
    "        \n",
    "        if (len(df))< 10000 :\n",
    "            print (df['date'] )\n",
    "    else :\n",
    "        df = tweets \n",
    "        condition = df['date'] == '2017-01-'+str(i)\n",
    "        df = df[condition]\n",
    "        print (len(df))\n",
    "        \n",
    "        if (len(df))< 10000 :\n",
    "            print (df['date'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the text\n",
    "tweets['text'] = tweets['tweet'].map(clean_tweet)\n",
    "\n",
    "# 1 if more than 10 likes\n",
    "tweets['A_Lot_Likes'] = tweets['likes'].map(alot_likes)\n",
    "\n",
    "# 1 If have emoticon :) -1 otherwise and 0 at neutral\n",
    "tweets['Emoticons'] = tweets['text'].map(emoticons)\n",
    "\n",
    "#Polarity by vader\n",
    "tweets['polarity_vader'] = tweets['text'].map(vader)\n",
    "\n",
    "#Polarity by textblob\n",
    "tweets['polarity_textblob'] = tweets['text'].map(get_tweet_sentiment)\n",
    "\n",
    "# Create Timestamp\n",
    "tweets['timestamp'] = pd.to_datetime(tweets['date'] + ' ' + tweets['time'])\n",
    "tweets['timestamp'] = tweets['timestamp'].map(truncate_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[['text', 'timestamp', 'polarity_textblob', 'polarity_vader', 'A_Lot_Likes','Emoticons', 'hashtags']]\n",
    "tweets = tweets.rename(columns={'text': 'Text', 'polarity_textblob': 'Polarity_Textblob', 'timestamp' : 'Timestamp', 'Emoticons' : 'Emoticons',\n",
    "                               'A_Lot_Likes' : 'A_Lot_Likes','polarity_vader' : 'Polarity_Vader', 'hashtags' : 'Hashtags'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets_raw_day.csv')\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
